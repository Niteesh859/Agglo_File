{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('myopia.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, x, layer_config=[10, 10]):\n",
    "        super(Model, self).__init__()\n",
    "        layers = []\n",
    "        input_size = x\n",
    "        for hidden_neurons in layer_config:\n",
    "            # Add a linear layer\n",
    "            layers.append(nn.Linear(input_size, hidden_neurons))\n",
    "            # Add a ReLU activation\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_neurons\n",
    "        layers.append(nn.Softmax())\n",
    "        \n",
    "        # Combine all layers into a sequential module\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Example usage\n",
    "model0 = Model(x=10, layer_config=[20, 20])\n",
    "print(model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGAN\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is a Pandas DataFrame containing your full dataset\n",
    "# Specify which columns are categorical/binary if needed\n",
    "discrete_columns = ['AGE', 'GENDER', 'SPORTHR', 'READHR', 'COMPHR', 'STUDYHR', 'TVHR', 'DIOPTERHR', 'MOMMY', 'DADMY']\n",
    "\n",
    "ctgan = CTGAN(epochs=10)  # Increase epochs for better quality\n",
    "ctgan.fit(df, discrete_columns=discrete_columns)\n",
    "\n",
    "# Generate 1000 new synthetic samples\n",
    "synthetic_data = ctgan.sample(300)\n",
    "\n",
    "# Combine synthetic data with your original dataset for training\n",
    "df = pd.concat([df, synthetic_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Size: 688 (74.95%)\n",
      "Validation Data Size: 138 (15.03%)\n",
      "Test Data Size: 92 (10.02%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is `df`\n",
    "# Step 1: Split into training (75%) and remaining (25%)\n",
    "datatrain, temp_data = train_test_split(df, test_size=0.25, stratify=df['MYOPIC'], random_state=2)\n",
    "\n",
    "# Step 2: Split remaining data (25%) into validation (60% of remaining) and test (40% of remaining)\n",
    "dataval, datatest = train_test_split(temp_data, test_size=0.4, stratify=temp_data['MYOPIC'], random_state=21)\n",
    "\n",
    "x_train = datatrain[['AGE', 'GENDER', 'SPORTHR', 'READHR', 'COMPHR', 'STUDYHR', 'TVHR', 'DIOPTERHR', 'MOMMY', 'DADMY']]\n",
    "x_val = dataval[['AGE', 'GENDER', 'SPORTHR', 'READHR', 'COMPHR', 'STUDYHR', 'TVHR', 'DIOPTERHR', 'MOMMY', 'DADMY']]\n",
    "x_test = datatest[['AGE', 'GENDER', 'SPORTHR', 'READHR', 'COMPHR', 'STUDYHR', 'TVHR', 'DIOPTERHR', 'MOMMY', 'DADMY']]\n",
    "y_train = datatrain['MYOPIC']\n",
    "y_val = dataval['MYOPIC']\n",
    "y_test = datatest['MYOPIC']\n",
    "\n",
    "# Confirm the sizes\n",
    "print(f\"Train Data Size: {len(datatrain)} ({len(datatrain)/len(df)*100:.2f}%)\")\n",
    "print(f\"Validation Data Size: {len(dataval)} ({len(dataval)/len(df)*100:.2f}%)\")\n",
    "print(f\"Test Data Size: {len(datatest)} ({len(datatest)/len(df)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hyperparameters\n",
    "lr = 0.1   # Reduced learning rate for stability\n",
    "epochs = 50  # Increased epochs for better training\n",
    "batch_size = 64\n",
    "layers = [500, 500, 500, 500]\n",
    "optimizer = torch.optim.AdamW(model0.parameters(), lr=lr)\n",
    "dropout_factor = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 50/50 | Train Loss: 0.49270, Train Acc: 80.67%, Val Loss: 0.49414, Val Acc: 81.16%, Avg Softmax (Train): 0.1862, Avg Diff (Train): 0.3077, Avg Softmax (Val): 0.2458, Avg Diff (Val): 0.33968.1884\n",
      "Test Loss: 0.44292, Test Acc: 80.43%, Avg Softmax (Test): 0.2513, Avg Diff (Test): 0.3486\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available and set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# 1. Custom Dataset definition\n",
    "#    Labels are float in [0,1].\n",
    "class DataTest(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x = torch.tensor(x_data.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y_data.values, dtype=torch.float32)\n",
    "        self.n_samples = len(x_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# 2. Model definition with random weight initialization\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_neurons, layer_config=layers, num_classes=2):\n",
    "        super(Model, self).__init__()\n",
    "        layers = []\n",
    "        input_size = input_neurons\n",
    "        for hidden_neurons in layer_config:\n",
    "            layers.append(nn.Linear(input_size, hidden_neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_factor))  # Dropout regularization\n",
    "            input_size = hidden_neurons\n",
    "        layers.append(nn.Linear(input_size, num_classes))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Apply He (Kaiming) initialization - Normal distribution\n",
    "                init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "                \n",
    "                # Initialize bias to zeros (optional)\n",
    "                if layer.bias is not None:\n",
    "                    init.constant_(layer.bias, 0)\n",
    "\n",
    "# 3. Numerically stable soft-label cross-entropy loss using log_softmax\n",
    "def soft_label_crossentropy(logits, label):\n",
    "    \"\"\"\n",
    "    logits: shape (batch_size, 2)\n",
    "    label: shape (batch_size,) with float values in [0,1]\n",
    "    \"\"\"\n",
    "    # Construct target distribution: [1 - y, y]\n",
    "    target_dist = torch.stack([1 - label, label], dim=1)  # shape [batch_size, 2]\n",
    "    # Compute log probabilities in a stable way\n",
    "    log_prob = F.log_softmax(logits, dim=1)\n",
    "    # Cross-entropy = -sum(target * log_prob) averaged over batch\n",
    "    loss_per_sample = -(target_dist * log_prob).sum(dim=1)\n",
    "    return loss_per_sample.mean()\n",
    "\n",
    "try:\n",
    "    train_dataset = DataTest(x_train, y_train)\n",
    "    val_dataset = DataTest(x_val, y_val)\n",
    "    test_dataset = DataTest(x_test, y_test)\n",
    "except NameError:\n",
    "    raise NameError(\"Please define x_train, y_train, x_val, y_val, and x_test, y_test before running the script.\")\n",
    "\n",
    "# 6. Create data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 7. Initialize model and optimizer\n",
    "input_features = x_train.shape[1]\n",
    "model0 = Model(input_neurons=input_features, layer_config=layers, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model0.parameters(), lr=lr)\n",
    "\n",
    "# 8. Training and validation loop\n",
    "for epoch in range(epochs):\n",
    "    # ---- Training Phase ----\n",
    "    model0.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    total_train_samples = 0\n",
    "    train_softmax_outputs = []\n",
    "    train_diff_sum = 0.0  # Sum of absolute differences\n",
    "\n",
    "    for data, label in train_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass: get logits\n",
    "        logits = model0(data)                # shape = (batch_size, 2)\n",
    "        \n",
    "        # Compute loss using logits directly\n",
    "        loss = soft_label_crossentropy(logits, label)\n",
    "\n",
    "        # Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Compute probabilities for accuracy and analysis\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        preds = (prob[:, 1] >= 0.5).float()    # threshold at 0.5 for binary decision\n",
    "        total_train_correct += (preds == label).sum().item()\n",
    "        total_train_samples += label.size(0)\n",
    "        train_softmax_outputs.extend(prob[:, 1].detach().cpu().tolist())\n",
    "        \n",
    "        # Compute absolute difference between label and predicted probability\n",
    "        train_diff_sum += torch.abs(prob[:, 1] - label).sum().item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = (total_train_correct / total_train_samples) * 100\n",
    "    avg_train_softmax = np.mean(train_softmax_outputs)\n",
    "    avg_train_diff = train_diff_sum / total_train_samples\n",
    "\n",
    "    # ---- Validation Phase ----\n",
    "    model0.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_samples = 0\n",
    "    val_softmax_outputs = []\n",
    "    val_diff_sum = 0.0  # Sum of absolute differences\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            logits = model0(data)\n",
    "            loss = soft_label_crossentropy(logits, label)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            prob = F.softmax(logits, dim=1)\n",
    "            preds = (prob[:, 1] >= 0.5).float()\n",
    "            total_val_correct += (preds == label).sum().item()\n",
    "            total_val_samples += label.size(0)\n",
    "            val_softmax_outputs.extend(prob[:, 1].cpu().tolist())\n",
    "            \n",
    "            # Compute absolute difference between label and predicted probability\n",
    "            val_diff_sum += torch.abs(prob[:, 1] - label).sum().item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = (total_val_correct / total_val_samples) * 100\n",
    "    avg_val_softmax = np.mean(val_softmax_outputs)\n",
    "    avg_val_diff = val_diff_sum / total_val_samples\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.5f}, Train Acc: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {avg_val_loss:.5f}, Val Acc: {val_accuracy:.2f}%, \"\n",
    "          f\"Avg Softmax (Train): {avg_train_softmax:.4f}, Avg Diff (Train): {avg_train_diff:.4f}, \"\n",
    "          f\"Avg Softmax (Val): {avg_val_softmax:.4f}, Avg Diff (Val): {avg_val_diff:.4f}\", end=\"\\r\")\n",
    "\n",
    "# ---- Testing Phase ----\n",
    "model0.eval()\n",
    "total_test_loss = 0.0\n",
    "total_test_correct = 0\n",
    "total_test_samples = 0\n",
    "test_softmax_outputs = []\n",
    "test_diff_sum = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        logits = model0(data)\n",
    "        loss = soft_label_crossentropy(logits, label)\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        preds = (prob[:, 1] >= 0.5).float()\n",
    "        total_test_correct += (preds == label).sum().item()\n",
    "        total_test_samples += label.size(0)\n",
    "        test_softmax_outputs.extend(prob[:, 1].cpu().tolist())\n",
    "        test_diff_sum += torch.abs(prob[:, 1] - label).sum().item()\n",
    "\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "test_accuracy = (total_test_correct / total_test_samples) * 100\n",
    "avg_test_softmax = np.mean(test_softmax_outputs)\n",
    "avg_test_diff = test_diff_sum / total_test_samples\n",
    "\n",
    "print(f\"\\nTest Loss: {avg_test_loss:.5f}, Test Acc: {test_accuracy:.2f}%, \"\n",
    "      f\"Avg Softmax (Test): {avg_test_softmax:.4f}, Avg Diff (Test): {avg_test_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.43%\n",
      "Avg Softmax (Test): [0.7487205862998962, 0.2512793242931366]\n"
     ]
    }
   ],
   "source": [
    "# Assuming x_test and y_test are pandas DataFrame and Series, respectively.\n",
    "# Convert them into a dataset and DataLoader for evaluation.\n",
    "\n",
    "# 1. Create a test dataset and data loader\n",
    "test_dataset = DataTest(x_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 2. Evaluation on the test set\n",
    "model0.eval()  # Set the model to evaluation mode\n",
    "total_test_correct = 0\n",
    "total_test_samples = 0\n",
    "test_softmax_outputs = []  # To collect softmax outputs for the test set\n",
    "\n",
    "with torch.no_grad():  # No gradient computation for evaluation\n",
    "    for data, label in test_loader:\n",
    "        data = data.to(torch.float32)\n",
    "        label = label.to(torch.long)\n",
    "\n",
    "        outputs = model0(data)\n",
    "\n",
    "        # Apply softmax to get probabilities and collect\n",
    "        softmax_outputs = F.softmax(outputs, dim=1).tolist()\n",
    "        test_softmax_outputs.extend(softmax_outputs)\n",
    "\n",
    "        # Predictions and accuracy\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        total_test_correct += (preds == label).sum().item()\n",
    "        total_test_samples += label.size(0)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = (total_test_correct / total_test_samples) * 100\n",
    "avg_test_softmax = torch.tensor(test_softmax_outputs).mean(dim=0)  # Average softmax outputs\n",
    "\n",
    "# Print test set results\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Avg Softmax (Test): {avg_test_softmax.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model state dictionary (recommended)\n",
    "torch.save(model0.state_dict(), 'Model0.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>STUDYYEAR</th>\n",
       "      <th>MYOPIC</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>SPHEQ</th>\n",
       "      <th>AL</th>\n",
       "      <th>ACD</th>\n",
       "      <th>LT</th>\n",
       "      <th>VCD</th>\n",
       "      <th>SPORTHR</th>\n",
       "      <th>READHR</th>\n",
       "      <th>COMPHR</th>\n",
       "      <th>STUDYHR</th>\n",
       "      <th>TVHR</th>\n",
       "      <th>DIOPTERHR</th>\n",
       "      <th>MOMMY</th>\n",
       "      <th>DADMY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.052000</td>\n",
       "      <td>21.890000</td>\n",
       "      <td>3.690000</td>\n",
       "      <td>3.498000</td>\n",
       "      <td>14.700000</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>22.380000</td>\n",
       "      <td>3.702000</td>\n",
       "      <td>3.392000</td>\n",
       "      <td>15.290000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1991</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.179000</td>\n",
       "      <td>22.490000</td>\n",
       "      <td>3.462000</td>\n",
       "      <td>3.514000</td>\n",
       "      <td>15.520000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>3.862000</td>\n",
       "      <td>3.612000</td>\n",
       "      <td>14.730000</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>23.290000</td>\n",
       "      <td>3.676000</td>\n",
       "      <td>3.454000</td>\n",
       "      <td>16.160000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>617</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915113</td>\n",
       "      <td>22.068130</td>\n",
       "      <td>3.135561</td>\n",
       "      <td>3.568084</td>\n",
       "      <td>14.190430</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>337</td>\n",
       "      <td>1993</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909335</td>\n",
       "      <td>23.192723</td>\n",
       "      <td>2.682925</td>\n",
       "      <td>3.408517</td>\n",
       "      <td>15.074215</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>242</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.677650</td>\n",
       "      <td>22.328070</td>\n",
       "      <td>3.375869</td>\n",
       "      <td>3.503506</td>\n",
       "      <td>17.208199</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>459</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.893230</td>\n",
       "      <td>23.379400</td>\n",
       "      <td>3.526105</td>\n",
       "      <td>3.927830</td>\n",
       "      <td>15.267251</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>631</td>\n",
       "      <td>1989</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.384172</td>\n",
       "      <td>22.245390</td>\n",
       "      <td>3.295179</td>\n",
       "      <td>3.843724</td>\n",
       "      <td>14.780832</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>918 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  STUDYYEAR  MYOPIC  AGE  GENDER     SPHEQ         AL       ACD  \\\n",
       "0      1       1992       1    6       1 -0.052000  21.890000  3.690000   \n",
       "1      2       1995       0    6       1  0.608000  22.380000  3.702000   \n",
       "2      3       1991       0    6       1  1.179000  22.490000  3.462000   \n",
       "3      4       1990       1    6       1  0.525000  22.200000  3.862000   \n",
       "4      5       1995       0    5       0  0.697000  23.290000  3.676000   \n",
       "..   ...        ...     ...  ...     ...       ...        ...       ...   \n",
       "913  617       1990       0    5       1  0.915113  22.068130  3.135561   \n",
       "914  337       1993       0    5       1  0.909335  23.192723  2.682925   \n",
       "915  242       1990       0    7       0  0.677650  22.328070  3.375869   \n",
       "916  459       1994       0    8       0  0.893230  23.379400  3.526105   \n",
       "917  631       1989       0    8       0  1.384172  22.245390  3.295179   \n",
       "\n",
       "           LT        VCD  SPORTHR  READHR  COMPHR  STUDYHR  TVHR  DIOPTERHR  \\\n",
       "0    3.498000  14.700000       45       8       0        0    10         34   \n",
       "1    3.392000  15.290000        4       0       1        1     7         12   \n",
       "2    3.514000  15.520000       14       0       2        0    10         14   \n",
       "3    3.612000  14.730000       18      11       0        0     4         37   \n",
       "4    3.454000  16.160000       14       0       0        0     4          4   \n",
       "..        ...        ...      ...     ...     ...      ...   ...        ...   \n",
       "913  3.568084  14.190430       14      12      10        4     9         15   \n",
       "914  3.408517  15.074215       29       8       2       12    23          2   \n",
       "915  3.503506  17.208199        7       7      11        4    22          9   \n",
       "916  3.927830  15.267251        8      15      12       12    25         11   \n",
       "917  3.843724  14.780832       23       3       5        4    12         90   \n",
       "\n",
       "     MOMMY  DADMY  \n",
       "0        1      1  \n",
       "1        1      1  \n",
       "2        0      0  \n",
       "3        0      1  \n",
       "4        1      0  \n",
       "..     ...    ...  \n",
       "913      0      0  \n",
       "914      0      0  \n",
       "915      0      1  \n",
       "916      0      0  \n",
       "917      1      0  \n",
       "\n",
       "[918 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully: {'survey': [{'age': 19, 'gender': 'Male', 'id': 1, 'name': 'Y Niteesh Reddy', 'time_reading': 44, 'time_sports': 23, 'time_studying': 86, 'time_tv': 5123456789, 'time_video_games': 323}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Backend API URL\n",
    "url = \"http://127.0.0.1:5000/survey\"  # Replace 'endpoint' with your actual route\n",
    "\n",
    "try:\n",
    "    # Make a GET request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        datai = response.json()\n",
    "        print(\"Data fetched successfully:\", datai)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "        print(\"Response:\", response.text)\n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'survey': [{'age': 19,\n",
       "   'gender': 'Male',\n",
       "   'id': 1,\n",
       "   'name': 'Y Niteesh Reddy',\n",
       "   'time_reading': 44,\n",
       "   'time_sports': 23,\n",
       "   'time_studying': 86,\n",
       "   'time_tv': 5123456789,\n",
       "   'time_video_games': 323}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
